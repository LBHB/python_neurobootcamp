{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Further work with confocal image datasets\n",
    "\n",
    "## 4.1 - Overview\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Yesterday you worked with some statistics extracted from an Imaris file. Today,\n",
    "you will learn how to work directly with the file to extract additional\n",
    "information. \n",
    "\n",
    "Imaris is an open-source file format based on HDF5. A number of languages have\n",
    "packages for reading this file format. For Python, there are two main packages,\n",
    "`h5py` and `pytables`. We will use `pytables` for this exercise.\n",
    "\n",
    "### The data\n",
    "\n",
    "Up to 25 auditory nerve fibers synapse onto individual inner hair cells in\n",
    "normal-hearing individuals. However, these synapses can be permanently lost due\n",
    "to aging, exposure to noise or ototoxic drugs.  In experiments that study\n",
    "hearing loss, we need a way of quantifying the number of synapses per inner\n",
    "hair cell.\n",
    "\n",
    "One approach is to dissect the cochlea out of the experimental animals and use\n",
    "whole-mount immunohistochemistry to label the tissue with antibodies for\n",
    "pre-synaptic ribbons (CtBP2), post-synaptic receptors (GluR2) and cytoskeleton\n",
    "(Myosin VIIa). In a second step, each antibody is tagged with a fluorescent dye\n",
    "that can be illuminated using a laser (much like how a black light can cause\n",
    "certain materials to glow).\n",
    "\n",
    "The distribution of these fluorescent dyes (which map to the underlying\n",
    "distribution of the proteins of interest) can be captured by taking a series of\n",
    "two-dimensional images at various depths in the tissue.  These images are then\n",
    "\"stacked\" to create a three-dimensional image known as a Z-stack (since the\n",
    "third dimension is commonly referred to as the Z-axis).\n",
    "\n",
    "For this exercise, the dataset has been trimmed down to a small subset showing\n",
    "only two inner hair cells (the full dataset is 0.5 GB in size) with CtBP2 (fig.\n",
    "1a) and GluR2 (fig. 1b).\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>1A. CtBP2 (pre-synaptic ribbon)</td>\n",
    "\t\t\t<td>1B. GluR2 (post-synaptic glutamate receptor)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"data/CtBP2.png\" /></td>\n",
    "\t\t\t<td><img src=\"data/GluR2.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "###  The problem\n",
    "\n",
    "A functional inner hair cell synapse requires both a pre-synaptic ribbon and a\n",
    "post-synaptic glutamate receptor. The next step in our analysis is to determine\n",
    "whether each CtBP2 puncta is near a GluR2 label. \n",
    "\n",
    "This dataset was analyzed using Imaris to identify all CtBP2 puncta (white dots\n",
    "in fig. 2a). If you look closely at the composite (fig. 2b), you'll see that\n",
    "not all puncta have a glutamate receptor patch next to them (fig. 2b)! We\n",
    "should not be counting these for the purpose of analysis. So, we need to find a\n",
    "way to detect these false hits and eliminate them.\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>A. CtBP2 puncta</td>\n",
    "\t\t\t<td>B. CtBP2 puncta overlaid on GluR2</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"data/CtBP2+points.png\" /></td>\n",
    "\t\t\t<td><img src=\"data/CtBP2+GluR2+points.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "One approach is to extract a fixed volume around each CtBP2 puncta (e.g., a 1um\n",
    "cube) and quantify the amount of GluR2 label in the volume. But, we don't know\n",
    "very much about the format of the data. We need to do a little exploration\n",
    "first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Getting started\n",
    "\n",
    "First, let's import a few modules we'll need. Most of them are common third-party modules; however, I have written a helper module (`imaris`) to extract some of the data on the CtBP2 puncta from the file (you loaded this data from a comma-separated-values file yesterday; however, we are now going to load it directly from the HDF5 file itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import tables as tb\n",
    "import numpy as np\n",
    "\n",
    "import imaris\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's open the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = tb.open_file('data/confocal dataset.ims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to figure out how to find the image data. We can start by taking a look at the file. Since HDF5 is a hierarchial data format, data inside the file is stored in a tree-like structure. We can view this structure using `print`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot of information! However, in scanning the list there are several\n",
    "things that jump out as important clues. First, remember that the tissue has\n",
    "three labels (CtBP2, GluR2 and MyosinVIIa). In confocal imaging, each label is\n",
    "acquired using a separate channel. At the bottom of the list we see several\n",
    "rows that mention `Channel 0`, `Channel 1` and `Channel 2`. This is most likely\n",
    "the data we need.\n",
    "\n",
    "However, the channels appear several times (under `ResolutionLevel 0`,\n",
    "`ResolutionLevel 1`). Which one do we want? Our\n",
    "intuition as a programmer tells us that Imaris likely generates the dataset at\n",
    "multiple resolutions and uses the appropriate resolution based on your zoom\n",
    "level. For quantitative analysis, we probably want the highest resolution\n",
    "level. \n",
    "\n",
    "Take another look at the list. You'll notice that at the end of each line\n",
    "there's an indicator in parenthesis (`Group`, `Array`, `CArray`). These are the\n",
    "different types of nodes (i.e., entries) in the HDF5 file. The simplest way to\n",
    "think of a HDF5 file is that it's a self-contained filesystem. A `group` node\n",
    "is equivalent to a folder. A `leaf` node (e.g., `Array`, `CArray`, `Table`) is\n",
    "equivalent to a file. Group nodes are used to organize the data in the HDF5\n",
    "file.\n",
    "\n",
    "Now, let's look at the `Channel 0/Data` line for each resolution level. There's\n",
    "some information about the size of the array. This tells us that\n",
    "`ResolutionLevel 0` contains the highest resolution data and `ResolutionLevel\n",
    "1` contains the lowest resolution data. Otherwise, they should be identical.\n",
    "\n",
    "Let's take a look at a single node so we can understand how to work with\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 0/Data')\n",
    "data_CtBP2 = node.read()\n",
    "print(data_CtBP2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3D `numpy` array containing the image data. Each element in the array represents a voxel (i.e., a 3D pixel). The first dimension is the Z-axis, second dimension the Y-axis and third (last) dimension the X-axis. \n",
    "For example, to pull out the pixel located at XYZ coordinates (60, 20, 50), you can index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CtBP2[50, 20, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ways to visualize 3D data in Python. However, these approaches are\n",
    "not readily available out of the box for Jupyter notebooks. Let's focus on simple 2D plotting\n",
    "instead. A common way of presenting confocal image stacks is to take the\n",
    "maximum projection along an an axis (i.e., dimension). Let's take the maximum\n",
    "projection along the first axis (i.e., Z-axis) and plot the resulting 2D image using `imshow`.\n",
    "\n",
    "The `origin='lower left'` argument to `pl.imshow` indicates that the data at `projection[0, 0]` should appear at the lower left corner of the axes instead of the default location (the upper left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = data_CtBP2.max(axis=0)\n",
    "pl.imshow(projection, origin='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Exercise - cropping the dataset\n",
    "\n",
    "It looks like the image has been \"padded\" with empty data by Imaris, making it a bit ugly to look at. Let's crop out that extra data. To do this, we need to find out what the actual image extents are in pixels. There is a way to do this by looking at the HDF5 file, but this is outside the scope of the exercise. For now, we provide the numbers for you.\n",
    "\n",
    "Using these numbers, use Numpy indexing to crop out the empty regions and replot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pixels = 161\n",
    "y_pixels = 194\n",
    "z_pixels = 135\n",
    "\n",
    "# Your solution for the exercise:\n",
    "## crop the dataset\n",
    "## compute the maximum projection of the cropped dataset\n",
    "## plot the new maximum projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Exercise - understanding the documentation\n",
    "\n",
    "The units on the X and Y-axes are in pixels. Let's convert them to actual image dimensions (in microns). First, you need to know the actual dimensions (this can be done by looking at the HDF5 file, but we provide the numbers for you). These are the the dimensions of the cropped dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_um = 22.7418\n",
    "y_um = 27.442\n",
    "z_um = 21.526"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember how to get help on a function? If not, go back and take a look at *insert reference to section here*. Take a look at the documentation for `pl.imshow`. Any clues as to what arguments can be used to get `imshow` to properly map each pixel to it's spatial location in microns? As a bonus, be sure to label the X and Y axes too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution for exercise\n",
    "## update imshow with the needed argument to plot the axes correctly\n",
    "## label the x and y axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Pulling in extra data\n",
    "\n",
    "Now, we need to load the data about the CtBP2 puncta that were identified using Imaris. Specifically, we need to know the XYZ location of each puncta. A helper function is provided to extract this information from the Imaris file and return it as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_CtBP2 = imaris.load_node_stats(fh, 'CtBP2', 'point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 - Exercise - overlaying a scatterplot on the image\n",
    "\n",
    "Our goal is to take the plot we've created using `imshow` with the axes showing the correct spatial location in microns and overlay a scatterplot showing the location of each CtBP2 puncta identified by Imaris.\n",
    "\n",
    "You've already learned how to inspect the contents of a dataframe (for a reminder, see *reference to section*). Take a look at the dataframe. What type of information does it have? What are the units (e.g., pixels or microns)?\n",
    "\n",
    "Once you have figured out how to obtain the X and Y coordinates for each puncta, you can plot them using `pl.plot(x_coordinates, y_coordinates, 'r+')` (the `'r+'` specifies a red cross marker). Do the coordinates align with the puncta observed in the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution to exercise\n",
    "## figure out what you need to pull out of the dataframe to plot the x and y coordinates of each puncta\n",
    "## cut and paste your answer from the previous exercise to plot the image\n",
    "## add the pl.plot command to plot the puncta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 - Extracting data from each image\n",
    "\n",
    "We want to use this data to extract a 1$\\mu m$ x 1$\\mu m$ x 1$\\mu m$ cube centered around each puncta. To do this we need to convert from $\\mu m$ to pixels. Since we know the dimensions in pixels and $\\mu m$, we can calculate the size of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size = x_um/x_pixels\n",
    "y_size = y_um/y_pixels\n",
    "z_size = z_um/z_pixels\n",
    "print(x_size, y_size, z_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that each pixel along the X and Y axes are 0.14 microns and the Z axis is 0.16 microns. If we want to convert from microns to pixels, we can simply divide by the pixel size. This means that a 1$\\mu m$ x 1$\\mu m$ x 1$\\mu m$ cube is approximately 7 x 7 x 6 pixels in size (rounded to the nearest pixel). For simplicity, let's assume that the cube should be 7 x 7 x 7 pixels in size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 - Exercise \n",
    "\n",
    "Now that you know how to convert from microns to pixels, let's pull out the first puncta in the dataframe and plot the maximum projection of the 1 x 1 x 1 $\\mu m$ region centered around the puncta.\n",
    "\n",
    "If you don't remember how to extract the first row of a dataframe, see *insert reference here*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise\n",
    "## Extract first row of dataframe\n",
    "## Convert coordinates stored in first row of dataframe to pixels.\n",
    "## Extract cube from `data_CtBP2`. Be sure to verify its size is 7 x 7 x 7.\n",
    "## Compute maximum projection along z-axis and plot it. Ensure axes are labeled appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 - Exercise (doing the same for GluR2)\n",
    "\n",
    "Looks like we've adequately identified the cube we need. Now, let's load the GluR2 data so we can plot the amount of GluR2 signal within this region as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 1/Data')\n",
    "data_GluR2 = node.read()\n",
    "data_GluR2 = data_GluR2[:z_pixels, :y_pixels, :x_pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise (cut and paste your solution from the prior exercise \n",
    "# and adapt it to work with data_GluR2 instead of data_CtBP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 - Quantifying the GluR2 signal\n",
    "\n",
    "Looks like there's some GluR2 signal next to the CtBP2 signal. Great! Now how do we quantify this? Maybe we can just take the average intensity within this GluR2 subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_GluR2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 - Exercise\n",
    "\n",
    "The next step would be to loop through each row (i.e., puncta) in the dataframe and extract the mean GluR2 signal. This can then be saved back as a new column in the dataframe. We can loop through the rows using the `iterrows` method. Flesh out the for loop below to create a list of the GluR2 signal intensity near each puncta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = []\n",
    "for _, puncta in stats_CtBP2.iterrows():\n",
    "    # Write code to extract cube from `data_GluR2` and compute mean value\n",
    "    signal.append(mean_GluR2_signal)\n",
    "    \n",
    "# Here, we can save the GluR2 signal back to the statistics dataframe as a new column\n",
    "stats_CtBP2['GluR2'] = signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 - Exercise\n",
    "\n",
    "Now, let's plot a histogram of the GluR2 signal near each CtBP2 puncta. This was covered in *insert reference*. Look at the histogram. Are there any obvious outliers? Is there an obvious cutoff threshold? Based on this, how many functional synapses are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's an obvious cutoff threshold we can use (i.e., < 10). How many functional synapses are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Creating a composite image\n",
    "\n",
    "In the above images, `imshow` is using a color map in which purple reflects the regions with no signal and yellow reflects regions with the most signal. But, what if we'd like to merge the three channels into a single image where red is mapped to CtBP2, green to GluR2 and blue to MyosinVIIa. How can we do this? Let's take another look at the documentation for `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.imshow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks `imshow` can take a 3D array where the last dimension maps to the three colors (i.e., `x[..., 0]` is red, `x[..., 1]` is green and `x[..., 2]` is blue). The documentation also warns that the values in the array must be in the range 0 ... 1 for this to work. Let's check that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 0/Data')\n",
    "data_CtBP2 = node.read()\n",
    "data_CtBP2 = data_CtBP2[:z_pixels, :y_pixels, :z_pixels]\n",
    "node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 1/Data')\n",
    "data_GluR2 = node.read()\n",
    "data_GluR2 = data_GluR2[:z_pixels, :y_pixels, :z_pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CtBP2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh. We need to fix that. The simplest way to coerce data to the range 0 ... 1 is to divide by the maximum value. Let's do this and check that we did OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_CtBP2 = data_CtBP2/np.max(data_CtBP2)\n",
    "data_GluR2 = data_GluR2/np.max(data_GluR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CtBP2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we need to make the 2D image for each color and then merge them into a 3D array. A list of 2D images can be stacked into a 3D array using Numpy's `dstack` function. We need to make a blank image for the blue color. The quickest way to do this is to use the `zeros_like` function from Numpy which will create an array of the same shape, but filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_CtBP2 = data_CtBP2.max(axis=0)\n",
    "projection_GluR2 = data_GluR2.max(axis=0)\n",
    "projection_blue = np.zeros_like(projection_CtBP2)\n",
    "\n",
    "data = [projection_CtBP2, projection_GluR2, projection_blue]\n",
    "projection = np.dstack(data)\n",
    "\n",
    "pl.imshow(projection, extent=extent)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

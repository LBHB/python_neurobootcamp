{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Further work with confocal image datasets\n",
    "\n",
    "<a name=\"outline\"></a>\n",
    "\n",
    "## Outline of this notebook\n",
    "\n",
    "* <a href=\"#4.0\">4.0 - Loading the data</a>\n",
    "\n",
    "\n",
    "* <a href=\"#4.1\">4.1 - Indexing Numpy arrays and plotting them</a>\n",
    "\n",
    "    * <a href=\"#exercise-projection\"> Exercise - Calculate the maximum projection </a>\n",
    "\n",
    "    * <a href=\"#exercise-cropping\">Exercise - Cropping the dataset and plotting it</a>\n",
    "\n",
    "    * <a href=\"#exercise-documentation\">Exercise - Using the documentation to figure out how to modify plotting behavior</a>\n",
    "\n",
    "\n",
    "* <a href=\"#4.2\">4.2 - Using statistics from file to analyze image</a>\n",
    "\n",
    "    * <a href=\"#exercise-overlaying\">Exercise - Overlaying a scatterplot on the image</a>\n",
    "\n",
    "    * <a href=\"#exercise-extract\">Exercise - Extract a fixed volume relative to puncta coordinates</a>\n",
    "\n",
    "    * <a href=\"#exercise-repeat\">Exercise - Repeat for the receptor signal</a>\n",
    "\n",
    "\n",
    "* <a href=\"#4.3\">4.3 - Quantifying the receptor signal</a>\n",
    "\n",
    "    * <a href=\"#exercise-calculate\">Exercise - Calculate mean receptor signal near each puncta</a>\n",
    "\n",
    "    * <a href=\"#exercise-histogram\">Exercise - Plot histogram of receptor signal and count number of functional synapses</a>\n",
    "\n",
    "\n",
    "* <a href=\"#4.4\">4.4 - Bonus - Creating a composite image</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Over the previous two days you've learned how to work extensively with DataFrames, a type of object provided by the Pandas library. Today, you will work with Numpy arrays. Much like DataFrames and lists, arrays are use to store sets of data.  Also, like dataframes and lists, arrays allow you to pull out subsets of data using slicing. Unlike Pandas DataFrames which are designed to work with tabular data, Numpy arrays are designed to work with multidimensional data. This makes them especially well suited for confocal datasets, which often contain three-dimensional image data. We'll talk more about this in a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data format\n",
    "\n",
    "Imaris is a third-party program that facilitates analysis of data acquired from a confocal and saves it in an open-sorce format based on HDF5. HDF5 stands for hierarchial data format version 5. A number of programming languages have packages for reading this file format. There are two main packages for Python, `h5py` and `pytables`. We will use `pytables` today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "Up to 25 auditory nerve fibers synapse onto individual inner hair cells in\n",
    "normal-hearing individuals. However, these synapses can be permanently lost due\n",
    "to aging, exposure to noise or ototoxic drugs.  In experiments that study\n",
    "hearing loss, we need a way of quantifying the number of synapses per inner\n",
    "hair cell.\n",
    "\n",
    "One approach is to dissect the cochlea out of the experimental animals and use\n",
    "whole-mount immunohistochemistry to label the tissue with antibodies for\n",
    "pre-synaptic ribbons (CtBP2), post-synaptic receptors (GluR2) and cytoskeleton\n",
    "(Myosin VIIa). In a second step, each antibody is tagged with a fluorescent dye\n",
    "that can be illuminated using a laser (much like how a black light can cause\n",
    "certain materials to glow).\n",
    "\n",
    "The distribution of these fluorescent dyes (which map to the underlying\n",
    "distribution of the proteins of interest) can be captured by taking a series of\n",
    "two-dimensional images at various depths in the tissue.  These images are then\n",
    "\"stacked\" to create a three-dimensional image known as a Z-stack (since the\n",
    "third dimension is commonly referred to as the Z-axis).\n",
    "\n",
    "For this exercise, the dataset has been trimmed down to a small subset showing\n",
    "only two inner hair cells (the full dataset is 0.5 GB in size) with CtBP2 (fig.\n",
    "1a) and GluR2 (fig. 1b).\n",
    "\n",
    "**For simplicity, we will refer to the CtBP2 label as \"ribbon\" and the GluR2 label as \"receptor\".**\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>1A. CtBP2 (pre-synaptic ribbon)</td>\n",
    "\t\t\t<td>1B. GluR2 (post-synaptic glutamate receptor)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"data/CtBP2.png\" /></td>\n",
    "\t\t\t<td><img src=\"data/GluR2.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "During confocal acquisition the signal intensity of each label is acquired as a separate **channel**. For each label, we have a separate set of three-dimensional image data representing the signal intensity for that label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The problem\n",
    "\n",
    "This dataset was analyzed using Imaris to identify all ribbon puncta (white dots\n",
    "in fig. 2a). If you look closely at the composite (fig. 2b), you'll see that\n",
    "not all puncta have a glutamate receptor patch next to them (fig. 2b). \n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>A. CtBP2 puncta</td>\n",
    "\t\t\t<td>B. CtBP2 puncta overlaid on GluR2</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"data/CtBP2+points.png\" /></td>\n",
    "\t\t\t<td><img src=\"data/CtBP2+GluR2+points.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "A functional inner hair cell synapse requires both a pre-synaptic ribbon and a\n",
    "post-synaptic glutamate receptor. The next step in our analysis is to determine\n",
    "whether each ribbon puncta is near a receptor.\n",
    "\n",
    "One approach is to extract a fixed volume around each ribbon puncta (e.g., a 1um\n",
    "cube) and quantify the amount of receptor label in that volume. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.0\"></a>\n",
    "## 4.0 - Loading the data\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "First, let's import a few modules we'll need. Remember that a Python module is basically a Python file (or collection of files) that contain useful functions that you can use in your code. \n",
    "\n",
    "Most of them are common third-party modules; however, I have written a helper module (`imaris`) to extract some of the statistics (calculated by Imaris). You loaded these statistics from a comma-separated-values file on day 2; however, we are now going to load it directly from the HDF5 file itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Popular third-party libraries\n",
    "import matplotlib.pyplot as pl\n",
    "import tables as tb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Custom module written for this exercise\n",
    "import imaris\n",
    "\n",
    "# This is a special \"magic\" command that can be used in Jupyter Notebooks.\n",
    "# It ensures that Matplotlib shows the plot below each cell (you won't see\n",
    "# a plot otherwise).\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the default precision for printing data in Numpy arrays.\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's open the file using the `pytables` library. Even though the library\n",
    "is called `pytables`, it installs itself in Python as `tables` and we imported\n",
    "it as `tb`.\n",
    "\n",
    "Here, `tb` is a module and `open_file` is a function defined in the module. The\n",
    "`open_file` function takes the path to the file and returns a PyTables `File`\n",
    "object.\n",
    "\n",
    "You've already worked extensively with objects before. A `DataFrame` is an\n",
    "object. Even a simple integer is an object! Objects can be thought of as a\n",
    "collection of data (i.e., attributes on the object) and functions (i.e.,\n",
    "methods on the object) that operate on this data.\n",
    "\n",
    "Here, `fh` is a `File` object that knows where the data is stored on disk, but\n",
    "it hasn't actually loaded the data itself. That's a good thing. HDF5 files can\n",
    "be several gigabytes in size (most computers only have 8 gigabytes of memory).\n",
    "However, it has methods that help you load the data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = tb.open_file('data/confocal dataset.ims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? If you've never worked with a particular type of object before, you\n",
    "can explore it a number of ways. We've already discussed two ways you can\n",
    "explore them. What are they?\n",
    "\n",
    "Another way is to just type `print(fh)`. What actually gets printed depends on\n",
    "how the developers of the library implement it for their particular object.\n",
    "Sometimes you don't get anything useful. Fortunately, the `PyTables` developers\n",
    "decided to produce useful output when you call the `print` function on a `File`\n",
    "object.\n",
    "\n",
    "Go ahead, try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot of information. As you can see, it nicely illustrates the\n",
    "structure of a HDF5 file. Recall how I mentioned HDF5 was short for hierarchial\n",
    "data format? You can see that the data in the file is nicely organized into\n",
    "what looks like a file-path like structure (e.g., `/Scene8/Content/Points0`).\n",
    "Much like your computer filesystem, the data is organized into \"folders\" (known\n",
    "as \"groups\" in HDF5 parlance) and \"files\" (known as \"nodes\" in HDF5 parlance).\n",
    "It's almost like a filesystem within a file.\n",
    "\n",
    "We're interested in finding the actual three-dimensional image data. Let's look\n",
    "through the information shown above. Does anything jump out as a clue as to where the data might be stored in the file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In confocal imaging, each label is acquired using a separate channel. At the\n",
    "bottom of the list we see several rows that mention `Channel 0`, `Channel 1`\n",
    "and `Channel 2`. This is most likely the data we need.\n",
    "\n",
    "However, the channels appear several times (under `ResolutionLevel 0` and\n",
    "`ResolutionLevel 1`). Which one do we want? Our intuition as a programmer tells\n",
    "us that Imaris likely generates the dataset at multiple resolutions and uses\n",
    "the appropriate resolution based on your zoom level. For quantitative analysis,\n",
    "we probably want the highest resolution level. \n",
    "\n",
    "Take another look at the list. You'll notice that at the end of each line\n",
    "there's an indicator in parenthesis (`Group`, `Array`, `CArray`).  different\n",
    "types of nodes in the HDF5 file. \n",
    "\n",
    "Now, let's look at the `Channel 0/Data` line for each resolution level. There's\n",
    "some information about the size of the array. This tells us that\n",
    "`ResolutionLevel 0` contains the highest resolution data and `ResolutionLevel 1` \n",
    "contains the lowest resolution data. Otherwise, they should be identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "## 4.1 - Indexing Numpy arrays and plotting them\n",
    "\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Let's take a look at `Channel 0` so we can understand how to work with\n",
    "the data. Based on the acquisition settings, channel 0 is the ribbon label and channel 1 is the receptor label. Let's start with the ribbon label.\n",
    "\n",
    "The file object has a method, `get_node` that returns a node object. This node object provides a method, `read`, that loads the data from disk and returns it as a Numpy array object.\n",
    "\n",
    "The array object has a `shape` attribute that tells you the size of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 0/Data')\n",
    "data_ribbon = node.read()\n",
    "print(data_ribbon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are three numbers. Pandas DataFrames only have two (the number\n",
    "of rows and columns). That's because DataFrames are two-dimensional. In\n",
    "contrast, we're working with three-dimensional image data. \n",
    "\n",
    "You've worked a little with Numpy arrays in the past two days. You can\n",
    "visualize a DataFrame as a spreadsheet table. Here's a way to visualize a 3D\n",
    "array.\n",
    "\n",
    "<img src=\"data/array_colour_slices.png\" />\n",
    "\n",
    "Since we're working with image data, each element in the array represents a\n",
    "voxel (i.e., a 3D pixel). The first dimension is the Z-axis, second dimension\n",
    "the Y-axis and third (last) dimension the X-axis. The ordering of the\n",
    "dimensions is how Imaris saves the data (for example, another program may\n",
    "choose to save the Z-axis as the last dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like lists and Pandas DataFrames, you can extract a subset of the data.\n",
    "For example, to pull out the pixel located at XYZ coordinates (60, 20, 50), you\n",
    "can use index notation. Remember how you do it with a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = ['a', 'b', 'c', 'd']\n",
    "print(x_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you do it with Numpy arrays. If the desired XYZ coordinates are (60, 20, 50), why does 50 appear first and 60 appear last in the notation below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ribbon[50, 20, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "If you want to extract a *range* of values, you can use the slice syntax. How do you extract the first two elements from a list? Do it for `x_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull out three elements from each dimension (this gives us a total of 3 x 3 x 3 = 27 values). As shown in the output below the cell, it looks like a set of 3 x 3 arrays that have been stacked 3-high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ribbon[50:53, 20:23, 60:63]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ways to visualize 3D data in Python. However, these approaches are\n",
    "not readily available out of the box for Jupyter notebooks. Let's focus on simple 2D plotting\n",
    "instead. A common way of presenting confocal image stacks is to take the\n",
    "maximum projection along an an axis (i.e., dimension). Let's take the maximum\n",
    "projection along the first axis (i.e., Z-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On day 2, we talked about how you can calculate statistics along an axis of a Numpy array. This was a very brief introduction to arrays, so let's refresh our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2D = np.random.rand(3, 2)\n",
    "print('Full dataset')\n",
    "print(data_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, a two-dimensional array has two axes: the first running vertically downwards across rows (axis 0) and the second running horizontally across columns (axis 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "We can pass an axis argument to the Numpy statistics functions (e.g., `mean`, `std`, `mean`) indicating which axis to operate across. If we take the mean of axis 0 (e.g., `np.mean(data_2D, axis=0)`) what is the shape of the result? What about when taking the mean of axis 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "In addition to using the `mean` function available through the Numpy module (called `np` in this notebook), there is also a `mean` method available through the array object that performs the same operation. Go ahead, try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a very simple 3D array with shape 4 x 3 x 2. When printing a 3D array, Numpy prints out each 2D section separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3D = np.random.rand(4, 3, 2)\n",
    "\n",
    "print('Full dataset')\n",
    "print(data_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What will the shape of the resulting array be if you take the mean of the first axis? Second axis? Third axis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-projection\"></a>\n",
    "### Exercise - calculate the maximum projection\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Now, calculate the maximum projection, along the Z-axis, of the confocal image. The maximum projection is the max value along that axis. You can either use the function, `np.max`, or the object method `data_ribbon.max`.\n",
    "\n",
    "Save it to a variable called `projection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note the shape of the projection. The Z-axis has been dropped, leaving us with a two-dimensional array that we can plot as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of full dataset', data_ribbon.shape)\n",
    "print('Shape of projection', projection.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot this 2D projection. The `origin='lower left'` argument to `pl.imshow` indicates that the data at `projection[0, 0]` should appear at the lower left corner of the axes instead of the default location (the upper left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.imshow(projection, origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-cropping\"></a>\n",
    "### Exercise - Cropping the dataset and plotting it\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "It looks like the image has been \"padded\" with empty data by Imaris, making it a bit ugly to look at. Let's crop out that extra data. To do this, we need to find out what the actual image extents are in pixels. There is a way to do this by looking at the HDF5 file, but this is outside the scope of the exercise. For now, we provide the numbers for you.\n",
    "\n",
    "Using these numbers, use Numpy indexing to crop out the empty regions and replot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pixels = 161\n",
    "y_pixels = 194\n",
    "z_pixels = 135\n",
    "\n",
    "# Your solution for the exercise:\n",
    "## crop the dataset\n",
    "## compute the maximum projection of the cropped dataset\n",
    "## plot the new maximum projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-documentation\"></a>\n",
    "### Exercise - Using the documentation to figure out how to modify plotting behavior\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "The units on the X and Y-axes are in pixels. Let's convert them to actual image dimensions (in microns). First, you need to know the actual dimensions (this can be done by looking at the HDF5 file, but we provide the numbers for you). These are the the dimensions of the cropped dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_um = 22.7418\n",
    "y_um = 27.442\n",
    "z_um = 21.526"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember how to get help on a function? Take a look at the documentation for `pl.imshow`. Any clues as to what arguments can be used to get `imshow` to properly map each pixel to it's spatial location in microns? As a bonus, be sure to label the X and Y axes too! You can use the `xlabel` and `ylabel` functions available through the matplotlib.pyplot module (`pl` in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution for exercise\n",
    "## update imshow with the needed argument to plot the axes correctly\n",
    "## label the x and y axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "## 4.2 - Using statistics from file to analylze image\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Now, we need to load the data about the ribbon puncta that were identified using Imaris. Specifically, we need to know the XYZ location of each puncta. A function from the `imaris` module called `load_node_stats` can be used to load this from the Imaris file. The function takes three arguments (the PyTables `File` object, the name of the label and the type of statistics to extract). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_ribbon = imaris.load_node_stats(fh, 'CtBP2', 'point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-overlaying\"></a>\n",
    "###  Exercise - overlaying a scatterplot on the image\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Our goal is to take the plot we created using `imshow` (with the axes showing the correct spatial location in microns) and overlay a scatterplot showing the location of each ribbon puncta identified by Imaris.\n",
    "\n",
    "You've already learned how to inspect the contents of a dataframe. Take a look at the dataframe. What type of information does it have? What are the units (e.g., pixels or microns)?\n",
    "\n",
    "Once you have figured out how to obtain the X and Y coordinates for each puncta, you can plot them using `pl.plot(x_coordinates, y_coordinates, 'r+')` (the `'r+'` specifies a red cross marker). Do the coordinates align with the puncta observed in the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution to exercise\n",
    "## figure out what you need to pull out of the dataframe to plot the x and y coordinates of each puncta\n",
    "## cut and paste your answer from the previous exercise to plot the image\n",
    "## add the pl.plot command to plot the puncta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use this data to extract a 1μm x 1μm x 1μm cube centered around each puncta. To do this we need to convert from microns to pixels. Since we know the dimensions in pixels and microns, we can calculate the size of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size = x_um/x_pixels\n",
    "y_size = y_um/y_pixels\n",
    "z_size = z_um/z_pixels\n",
    "\n",
    "print('Pixel size (x-axis) {:.2f}'.format(x_size))\n",
    "print('Pixel size (y-axis) {:.2f}'.format(y_size))\n",
    "print('Pixel size (z-axis) {:.2f}'.format(z_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel along the X and Y axes are 0.14 microns and the Z axis is 0.16 microns. If we want to convert from microns to pixels, we can divide by the pixel size. This means that a 1μm x 1μm x 1μm cube is approximately 7 x 7 x 6 pixels in size (rounded to the nearest pixel). For simplicity, let's assume that the cube should be 7 x 7 x 7 pixels in size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-extract\"></a>\n",
    "### Exercise - extract a fixed volume relative to puncta coordinates \n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Now that you know how to convert from microns to pixels, let's pull out the first puncta in the dataframe and plot the maximum projection of the 1μm³ region centered around the puncta.\n",
    "\n",
    "If you don't remember how to extract the first row of the dataframe, take a look at day 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, one small caveat. When indexing Numpy arrays (and also Python lists), you must use integers. Since divison returns floating point values, you need to cast the result to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 32/4\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_result = int(round(result))\n",
    "type(int_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, enter your solution below:\n",
    "* Convert coordinates stored in first row of dataframe to pixels. Don't forget to cast to an integer!\n",
    "* Extract cube from `data_ribbon`. Be sure to verify its size is 7 x 7 x 7.\n",
    "* Compute maximum projection along z-axis and plot it.\n",
    "* Ensure axes are labeled appropriately and reflect data coordinates, not pixel coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution to exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-repeat\"></a>\n",
    "### Exercise - repeat for the receptor signal\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Looks like we've adequately identified the cube we need. Now, let's load and crop the receptor data so we can plot the amount of receptor signal within this region as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 1/Data')\n",
    "data_receptor = node.read()\n",
    "data_receptor = data_receptor[:z_pixels, :y_pixels, :x_pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution to exercise (cut and paste your solution from the prior exercise \n",
    "# and adapt it to work with data_receptor instead of data_ribbon). Save the result\n",
    "# in a variable called `subset_receptor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.3\">\n",
    "## 4.3 - Quantifying the receptor signal\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "### Question\n",
    "Looks like there's some receptor signal next to the ribbon signal. Great! Now how do we quantify this? Maybe we can just take the average intensity within this subset? How do we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-calculate\"></a>\n",
    "### Exercise - Calculate mean receptor signal near each puncta\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "The next step is to loop through each row (i.e., puncta) in the dataframe and extract the mean receptor signal. This can then be saved back as a new column in the dataframe. We can loop through the rows using the `iterrows` method. On each cycle of the for loop, the `iterrows` method returns two values. The first value is the index of the row and the second value is the data in the row itself (stored in a dictionary-like format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal = []\n",
    "for index, puncta in stats_ribbon.iterrows():\n",
    "    # Write code to extract cube from `data_receptor` and compute mean value. Store\n",
    "    # it in a variable called `mean_receptor_signal`.\n",
    "    signal.append(mean_receptor_signal)\n",
    "    \n",
    "# Here, we can save the receptor signal back to the statistics dataframe as a new column.\n",
    "stats_ribbon['receptor'] = signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exercise-histogram\"></a>\n",
    "### Exercise - plot histogram of receptor signal and count number of functional synapses\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "Now, let's plot a histogram of the receptor signal near each ribbon puncta. Look at the histogram. Are there any obvious outliers? Is there an obvious cutoff threshold? Based on this, how many functional synapses are there? \n",
    "\n",
    "If you're not sure how to plot a histogram. Take a look at the documentation for the `matplotlib.pyplot` module (referred to as `pl` in this notebook). Any particular functions jump out at you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to plot the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "## 4.4 - Bonus - Creating a composite image\n",
    "<a href=\"#outline\">Jump to outline</a>\n",
    "\n",
    "In the above images, `imshow` is using a color map in which purple reflects the regions with no signal and yellow reflects regions with the most signal. But, what if we'd like to merge the three channels into a single image where red is mapped to the ribbon and green to the receptor. How can we do this? Let's take another look at the documentation for `imshow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks `imshow` can take a 3D array where the last dimension maps to the three colors (i.e., `x[..., 0]` is red, `x[..., 1]` is green and `x[..., 2]` is blue). The documentation also warns that the values in the array must be in the range 0 ... 1 for this to work. Let's check that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ribbon = data_ribbon[:z_pixels, :y_pixels, :z_pixels]\n",
    "data_receptor = data_receptor[:z_pixels, :y_pixels, :z_pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ribbon.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh. We need to fix that. The simplest way to coerce data to the range 0 ... 1 is to divide by the maximum value. Let's do this and check that we did OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_data_ribbon = data_ribbon/np.max(data_ribbon)\n",
    "norm_data_receptor = data_receptor/np.max(data_receptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_ribbon.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we need to make the 2D image for each color and then merge them into a 3D array. A list of 2D images can be stacked into a 3D array using Numpy's `dstack` function. We need to make a blank image for the blue color. The quickest way to do this is to use the `zeros_like` function from Numpy which will create an array of the same shape, but filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_ribbon = norm_data_ribbon.max(axis=0)\n",
    "projection_receptor = norm_data_receptor.max(axis=0)\n",
    "projection_blank = np.zeros_like(projection_ribbon)\n",
    "\n",
    "data = [projection_ribbon, projection_receptor, projection_blank]\n",
    "projection = np.dstack(data)\n",
    "\n",
    "pl.imshow(projection)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

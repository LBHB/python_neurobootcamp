{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Diving into Pandas\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/B_1KzLlUYAIFadB.jpg:large\" width=\"200\" height=\"200\" />\n",
    "\n",
    "## Outline of this notebook\n",
    "\n",
    "2.0 [Importing `pandas` and `numpy`](#importing)\n",
    "\n",
    "2.1 [Paths and Loading Data using `read_csv`](#paths)\n",
    "\n",
    "2.2 [Basic `DataFrame` manipulations](#df)\n",
    "\n",
    "2.3 [An Intro to Functions](#functions)\n",
    "\n",
    "2.4 [Grouping](#grouping)\n",
    "\n",
    "2.5 [Plotting](#plotting)\n",
    "\n",
    "2.6 [Boxplots](#boxplots)\n",
    "\n",
    "2.7 [Listing and looping over multiple files](#looping)\n",
    "\n",
    "2.8 [Concatenating a list of `DataFrame`s](#concatentating)\n",
    "\n",
    "2.9 [Discussion Question](#discuss)\n",
    "\n",
    "Exploring imaris image data\n",
    "\n",
    "2.9 [Exploring the Imaris Statistics](#imaris)\n",
    "\n",
    "2.10 [Pivoting](#pivoting)\n",
    "\n",
    "2.11 [Indexing](#indexing)\n",
    "\n",
    "2.12 [Plotting our Dataset](#plotting2)\n",
    "\n",
    "2.13 [Filtering](#filtering)\n",
    "\n",
    "2.14 [Discussion Question](#discuss2)\n",
    "\n",
    "2.15 [Summary](#summary)\n",
    "\n",
    "\n",
    "Appendix (optional sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Day 2! Python programmers often organize their code into chunks called modules. A module contains a set of related commands to accomplish a task. Today we are going to learn two of the most useful modules for data manipulation and plotting, `pandas` and `seaborn`. \n",
    "\n",
    "First we will use a simple data set on mouse weights taken from males and females of different strains to illustrate how to obtain simple descriptive statistics, group the data, and plot. \n",
    "\n",
    "Then, we will looking some imaging statistics data from Imaris and explore some more complex operations on the data.\n",
    "\n",
    "## 2.0 importing `pandas` and `numpy` <a name=\"importing\">\n",
    "\n",
    "To start, we'll load up two modules with the `import` statement. Modules are bits of reusable code that can either be coded in a file (which we'll see in day 4) or *installed* as a package. \n",
    "\n",
    "The first module we'll load is the `pandas` module, which will let us manipulate 2D tables. We refer to the `pandas` module here as `pd` as an abbreviation. The second module we load is the `numpy` module, mostly for the arithmetic functions built into it. We will do much more with the `numpy` module in Days 3 and 4.\n",
    "\n",
    "When you load a module using `import`, all of the functions available, such as `np.mean` are now accessible to you. Modules and import statements help programmers avoid naming conflicts because you can use short, straightforward names for functions and variables without worrying that they're already taken. Matlab does not have anything equivalent to Python's module system and therefore can be harder to read. \n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/python.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##Once imported, you can refer to the modules by `np`. \n",
    "##For example, we can take the mean of a list:\n",
    "np.mean([2,4,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paths\">\n",
    "## 2.1 Paths and Loading Data using `read_csv` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we even load the data, we need to tell Python where to find it. Our data is in the `data` directory in `day2`. We're going to specify the `path` or the folder location. There are two kinds of paths: *relative*, which is relative to our current directory, and *absolute*: the complete location of the directory. For example, the absolute path to the `data` folder on my Mac is: `/Users/laderast/Code/python_neurobootcamp/day2/data/`. \n",
    "\n",
    "The current directory for a Jupyter Notebook is always the directory where the notebook is located in. Since this notebook is in the `day2` directory, that is our current directory. All relative paths will be relative to this directory. You can check that `day2` is indeed your current working directory by using the `getcwd()` function in the `os` module. This will return the absolute path to your current directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#print \"Current Working Directory (cwd)\"\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to keep in mind about path setting:\n",
    "* If you are unsure of the path to your files, you can check by using a file explorer to navigate to the file, right click on it, and view properites. The path should be listed here \n",
    "* The separator (in this case it is a \"/\") will depend on your operating system: (\"/\" - Linux/mac, \"\\\" windows)\n",
    "* If you can, it's good practice to keep your Jupyter Notebook and your data in the same folder, and use relative paths to access everything. If someone wants to reproduce your analysis, they will be able to rerun everything in your notebook more easily. This makes your code more *reproducible*, which is really important these days. \n",
    "* IMPORTANT - Also be conscientious of how you name your files. This can be critical for writing robust, reliable scripts to analyze your data\n",
    "\n",
    "Okay, let's get to work! Let's load the data up using the `pd.read_csv` function. The `read_csv` function is a part of the `pandas` module, so we have to include the `pd.` in front of it so the computer knows to look in the `pandas` module to find and use this function. By using `pd.read_csv`, we return what is called a `pandas` `DataFrame`.   Most of our manipulations and plotting are going to be done on this `DataFrame`.\n",
    "\n",
    "A `DataFrame` can be thought of as a 2D table, but the values within each of the columns must be the same datatype. For example, any entry in the `MouseID` column must be a `string`, and any entry in the `Weight` column must be a `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mousedata = pd.read_csv(\"data/mouseData.csv\")\n",
    "##show the whole DataFrame\n",
    "mousedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Basic DataFrame manipulations<a name=\"df\">\n",
    "\n",
    "The first thing to do with our `DataFrame` is to look at the first few rows of the function using `head()`. We often do this just to confirm that we loaded the data correctly (that it has the correct column names).\n",
    "\n",
    "**Note**: the `.` notation lets us access built-in functions that are defined for any `DataFrame`. The built-in functions are also called *methods*. You can see all of the functions that you can do to a `DataFrame` by typing `mousedata.` into a code cell and hitting the `Tab` key. There's lots of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mousedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really useful method that we can immediately call on `mousedata` is `describe`. `describe` will return summary statistics on the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mousedata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful thing to note is the `.shape`, which returns the dimensions of the `DataFrame`. This can also be useful in confirming that we loaded the data in correctly.\n",
    "\n",
    "Note that we don't use `()` after `.shape`. This is because `shape` is a `Property` and not a `method` for `DataFrame`. This can be really confusing at first. Reading the documentation for `DataFrame` will tell you what's a method and what's a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mousedata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we'd like to get the number of rows in our `DataFrame`. Since `mousedata.shape` is a tuple (a special kind of list), we can access it using `[0]`. This can be really useful when we need to do something over all of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##get number of rows in mousedata\n",
    "mousedata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would we get the number of columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Put your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 A Quick Intro to functions<a name=\"functions\">\n",
    "\n",
    "Before we can move forward, we need to talk about functions a little bit. You can think of a function as a bit of reusable code. The important thing is that you need to define the inputs (what goes into the function) and the output (what comes out of the function).\n",
    "\n",
    "Try and run the following function. What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##always begin with \"def\" when defining a new function, \n",
    "##have an interface defined in the \"()\", \n",
    "##and the definition ends in \":\"\n",
    "def square_x(x):\n",
    "    out = x * x\n",
    "    return out\n",
    "\n",
    "square_x(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look a bit more closely at how the `square_x` function is written. It begins with the word `def` (short for define) followed by the name of the function, a variable named in parenthesis, and a colon. The variable is the input to the function. The colon is also a necessary part of the function definition, and it begins the code block that defines what the function does.\n",
    "\n",
    "The rest of the function consists of this code block. In Python, this block must be indented with either tabs or spaces. The last line in the block contains the word 'return' followed by a variable name. This variable is the output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make a new function called `cube_y` that takes `y` as an input, and returns the cube of `y`. \n",
    "\n",
    "Run `cube_y(2)` to test out your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## space for your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, we actually will want multiple inputs to our function, so we can do this by supplying more inputs to our function interface. Here we want to multiply two numbers together in our function, so we need to specify both `x` and `y` as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mult_xy(x, y):\n",
    "    out = x * y\n",
    "    return out\n",
    "\n",
    "mult_xy(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately there are many built-in functions for simple tasks, as you saw yesterday. For example, let's use the built-in `rand()` function from `numpy` to create a `DataFrame` with 10 columns of 100 random numbers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_m = np.random.rand(100,10)    # intitalize data matrix\n",
    "print(data_m.shape)                # 100 rows X 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use built-in function `mean()` to take the mean of all of these numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.mean(data_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, functions often have optional input values that allow you to use the function for more complicated tasks. For example, the documentation on `mean()` says that there is an optional input `axis` that allows you to compute means along a specific axis, i.e. columns (axis=0) or rows (axis=1). How would you modify the input above to compute means over the 10 colums?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Space for answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Grouping <a name=\"grouping\"\\>\n",
    "\n",
    "Let's get back to our `DataFrame`. We'll define a function and use it in conjunction with `groupby()`.  `groupby()` will group a `DataFrame` by the categories that exist in a column, and let you iterate through each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_mouse = mousedata.groupby(['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##return only those mice with Sex = \"M\"\n",
    "group_mouse.get_group('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you do with `groupby()`? One way to use it is to get *aggregate measures* based on group. For example, if we wanted to get the mean weight by gender, we can use the `apply` method on our data frames to return this. First we define a simple function called `mean_x` that returns the mean (we could have just used `np.mean` here, but it makes the code a little easier to understand).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_x(x):\n",
    "    return np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the `apply()` function to get the mean by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mousedata.groupby(['Sex']).apply(mean_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the `apply()` function takes a function as input and applies it to every element, in this case male and female weights.\n",
    "\n",
    "**Question**: why did the `groupby` only return `Weight`? (Hint: Does it make sense to do `mean_x` on our `Strain` variable?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Define a function to calculate the standard deviation (in `numpy` the function you need will be called `np.std`) and apply it to return the standard deviation of weights by `Strain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Space for your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Plotting <a name=\"plotting\">\n",
    "\n",
    "Let's look at some ways to visualize our `DataFrame`. We are going to use a module called `seaborn` to do our plotting, because the default plot options are pretty good, so we have to do less customization of our plots. We will also use some features from the module that `seaborn` is based on, `matplotlib`. You will see more of `matplotlib` tomorrow as well. \n",
    "\n",
    "Let's just plot the distribution of weights as a histogram. How many bins does our histogram have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import seaborn and matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "##we need this line in our notebook to make matplotlib/seaborn work with Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram of weights\n",
    "sns.distplot(mousedata.Weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Look up the help for `sns.distplot`. Note that there is a long list of input variables that are set as equal to `None` or `False`. This means that these are optional input variables that, unless defined, will run at their default definitions.\n",
    "\n",
    "To practice utilizing these optional inputs, change the number of bins to 40 in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(sns.distplot)\n",
    "\n",
    "## Space for your answer here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Boxplots (important!) <a name=\"boxplots\"/>\n",
    "\n",
    "Boxplots are super useful for looking at grouped means. Here we use the `sns.boxplot` function and group by `Sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "sns.boxplot(x = \"Sex\", y=\"Weight\", data=mousedata)\n",
    "\n",
    "# Set title with matplotlib\n",
    "plt.title('Mouse weight by sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "Create boxplots showing the weight data measured from the 2 different strains, B6 and D2. Make sure to add a title to your plot, such as \"Weight by Strain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Space for your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Listing and looping over multiple files (important!) <a name=\"looping\"/>\n",
    "\n",
    "One very important control structure to know is called a `for` loop. Basically, if we have a list of things, we can do the same thing to each element in the list. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_list = [1,4,9]\n",
    "\n",
    "for num in num_list:\n",
    "    print(\"I like the number \" + str(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note about the above code. \n",
    "\n",
    "1) The `for num in num_list:` line specifies how we refer to each element in the list. We're saying: let's grab an element from `num_list` and call it `num` for one cycle.\n",
    "\n",
    "2) The `:` is how we specify where the start of the code we want to do over and over again with each element.\n",
    "\n",
    "3) The code we want to do over and over again is *indented* under our `for` loop statement. The indentation is how we specify code blocks in python. \n",
    "\n",
    "4) Each cycle is called an *iteration*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what, why would we care? Remember when we used `pd.read_csv` above? We can do this for multiple files and return a `list` of `DataFrame`s. In order to do so, we'll use code in a module called `os` which will return a list of files in a directory. \n",
    "\n",
    "We're going to load files in the `data/maze_data` folder. These are experiments for mice running a maze with three replicates apiece.\n",
    "\n",
    "The first thing we need to do is get all the file names into a list using `os.listdir`. Then we can use `pd.read_csv` to load each of the files as a `DataFrame`, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/maze_data/\"\n",
    "file_list = os.listdir(path)\n",
    "file_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this list of file names? We can do the same thing over and over again over this list of file names, namely load each file as a `DataFrame`. We can do this with a `for` loop. \n",
    "\n",
    "`for` loops are a way to do something with each element of a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start out with an empty list\n",
    "mouse_list = []\n",
    "\n",
    "for fi in file_list:\n",
    "    full_path = path + fi\n",
    "    print(\"loading: \" + full_path)\n",
    "    ##load our file as a DataFrame\n",
    "    mouse_df = pd.read_csv(full_path, sep=\"\\t\")\n",
    "    ##add a file_name column to mouse_df\n",
    "    mouse_df[\"file_name\"] = fi\n",
    "    ##add our file into our list\n",
    "    mouse_list.append(mouse_df)\n",
    "    \n",
    "#Show the contents of mouse_list\n",
    "mouse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Concatenating a list of `DataFrame`s (important!) <a name=\"concatenating\"/>\n",
    "\n",
    "Okay, now we have a list of `DataFrame`s called `mouse_list`. We know that the column names are identical for each of our `DataFrame`s. Wouldn't it be easier if they were just one `DataFrame`?\n",
    "\n",
    "We can basically glue them into a single `DataFrame` using `pd.concat`, which will combine them by `row`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_mouse_frame = pd.concat(mouse_list)\n",
    "\n",
    "big_mouse_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "What if you just wanted the last two `DataFrame`s in our list to be concatenated? How would we do that?\n",
    "\n",
    "(Hint: think about subsetting your list. You should be able to do this in one line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Space for your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Discussion question <a name=\"discuss\"/>\n",
    "\n",
    "\n",
    "3. Why would we want to use `for` loops? Why are `for` loops useful for lists? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complicated example\n",
    "\n",
    "We're going to do much more manipulation and visualization with `pandas` using data taken from Imaris. Imaris is image analysis software with many sophisticated functions. Below is a confocal image taken of inner hair cells stained with antibodies against *CtBP2* (a pre-synaptic ribbon marker), *GluR2* (a post-synaptic receptor) and *MyosinVIIA* (which labels the entire hair cell). There are three color channels (red, green, and blue) which indicate the intensity of the staining for *CtBP2*, *GluR2* and *MyosinVIIa*, respectively.\n",
    "\n",
    "\n",
    "## The data\n",
    "\n",
    "Up to 25 auditory nerve fibers synapse onto individual inner hair cells in\n",
    "normal-hearing individuals. However, these synapses can be permanently lost due\n",
    "to aging, exposure to noise or ototoxic drugs.  In experiments that study\n",
    "hearing loss, we need a way of quantifying the number of synapses per inner\n",
    "hair cell.\n",
    "\n",
    "One approach is to dissect the cochlea out of the experimental animals and use\n",
    "whole-mount immunohistochemistry to label the tissue with antibodies for\n",
    "pre-synaptic ribbons (CtBP2), post-synaptic receptors (GluR2) and cytoskeleton\n",
    "(Myosin VIIa). The distribution of these proteins can be captured by taking a series of\n",
    "two-dimensional images at various depths in the tissue.  These images are then\n",
    "\"stacked\" to create a three-dimensional image known as a Z-stack (since the\n",
    "third dimension is commonly referred to as the Z-axis).\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>1A. CtBP2 (pre-synaptic ribbon)</td>\n",
    "\t\t\t<td>1B. GluR2 (post-synaptic glutamate receptor)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"../day-4/data/CtBP2.png\" /></td>\n",
    "\t\t\t<td><img src=\"../day-4/data/GluR2.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "## The problem\n",
    "\n",
    "A functional inner hair cell synapse requires both a pre-synaptic ribbon and a\n",
    "post-synaptic glutamate receptor. The next step in our analysis is to determine\n",
    "whether each CtBP2 puncta is near a GluR2 label. \n",
    "\n",
    "This dataset was analyzed using Imaris to identify all CtBP2 puncta (white dots\n",
    "in fig. 2a). If you look closely at the composite (fig. 2b), you'll see that\n",
    "not all puncta have a glutamate receptor patch next to them (fig. 2b)! We\n",
    "should not be counting these for the purpose of analysis. So, we need to find a\n",
    "way to detect these false hits and eliminate them.\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>A. CtBP2 puncta</td>\n",
    "\t\t\t<td>B. CtBP2 puncta overlaid on GluR2</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"../day-4/data/CtBP2+points.png\" /></td>\n",
    "\t\t\t<td><img src=\"../day-4/data/CtBP2+GluR2+points.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "One approach is to extract a fixed volume around each CtBP2 puncta (e.g., a 1um\n",
    "cube) and quantify the amount of GluR2 label in the volume. But, we don't know\n",
    "very much about the format of the data. We need to do a little exploration first.\n",
    "\n",
    "We used Imaris to detect all the \"spots\" in the CtBP2 (red) channel and compute some statistics about these spots. We've extracted the statistics file from the Imaris file into `csv` format just to make things easier. Just know that there are routines to extract this information from the file. Today we will practice exploring and visualizing the Imaris data and on Day 4 we will return to the question of how to quantify the amount of GluR2 label in a fixed volume around each CtBP2 puncta.\n",
    "\n",
    "Ok, let's load in the statistics for the CtBP2 puncta that were calculated by Imaris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats = pd.read_csv(\"data/points_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Exploring the Imaris Statistics <a name=\"imaris\"/>\n",
    "\n",
    "Because this data file was automatically generated by Imaris, we first need to figure out how it is organized.\n",
    "\n",
    "We can start taking a look at the first few rows of our summary table using `point_stats.head()`. In general, this is a really good practice to get into. Sometimes our data may have a header or not, and we may have loaded the data incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Show first few rows\n",
    "point_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some things we notice? Well, there appear to be some data that describe the entire sample (such as \"Total Number of Spots\") as well as data for localized points identified by Imaris in the red channel (such as \"Area\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##show last few rows\n",
    "point_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##show dimensions of data frame\n",
    "point_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that attributes for the various traits describing a given spot (such as \"Area\" and \"Volume\") are not columns, but rather listed under the categorical column \"Name.\" If we are curious to see this full list of names, use the unique() function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats.Name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back to the first few rows of data, it appears that ID_Object of -1 designates statistics that describe the entire sample. Let's confirm by viewing all rows with ID_Object of -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##we will talk more about the point_stats[\"ID_Object\"]==-1 notation in\n",
    "##section \n",
    "point_stats[point_stats[\"ID_Object\"]==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all of the statistics that were collected for a single spot identified by Imaris, starting with the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats[point_stats[\"ID_Object\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the raw data for Diameter of spots in the X dimension (\"Diameter X\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats[point_stats[\"Name\"]==\"Diameter X\"].head(20)\n",
    "#OR\n",
    "point_stats[point_stats[\"ID_StatisticsType\"]==237].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Pivoting <a name = \"pivoting\"/>\n",
    "\n",
    "Now let's create a `DataFrame` that is more intuitive in terms of viewing the statistics Imaris has collected for each identified spot in the red channel. We will call this DataFrame `point_stats_matrix`. To do this, use the `pivot()` function, which reshapes data based on column values. This function is extremely useful in transforming data from *long* format to *wide* format. \n",
    "\n",
    "The `pivot` method takes three arguments: `index`, which you can think of as being the rows of the data, `columns`, which specify what columns should exist in the data, and `values`, which are the actual numerical values we want in each Cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix = point_stats.pivot(index='ID_Object', columns='Name', values='Value')\n",
    "point_stats_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the statistics for the entire data set (including \"Number of spots per time point\" and \"Total number of spots\") have an ID_Object of -1. Let's remove this row, using the `.drop` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix = point_stats_matrix.drop(-1)\n",
    "point_stats_matrix.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `.loc()` when we want to access columns or rows by *name*. \n",
    "\n",
    "Remember when we were slicing the strings in Day 1? We use a very similar notation to access rows and columns in our `DataFrame`. Because the `DataFrame` has two dimensions, we need to specify two sets of positions to `loc`: *column* positions and *row* positions. We pass this on as a list of `row_positions` and `column_positions`.\n",
    "\n",
    "```\n",
    "point_stats.loc[[row_positions, column_positions]]\n",
    "```\n",
    "\n",
    "Let's use this notation to grab only the columns we want. Say we only want to view the `ID_Object`, `Value`, and `Name` columns from our `DataFrame`.\n",
    "\n",
    "Here we define a list called `colnames` and then pass as an argument into `.loc()`. Since we want all of the associated rows, we can use the `:` as an input to `row_positions`. `:` says, return all the values for the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = [\"Diameter X\", \"Diameter Y\", \"Diameter Z\"]\n",
    "\n",
    "diameter = point_stats_matrix.loc[:,colnames]\n",
    "\n",
    "diameter.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use our `mean_x` function to return the mean `Intensity Max X` from `point_stats_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Space for your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "There is actually a simpler way to create the diameter `DataFrame` that involves using regular expressions. Regular expressions are a very powerful way to specify searching over a `DataFrame`. Here we are just looking for entries with a certain word, but they can be tailored to search for all kinds of textual patterns.   \n",
    "\n",
    "We use the `filter()` function and tell it to grab all columns that contain the word \"Diameter.\" Axis can be either 0 (specifying rows) or 1 (specifying columns). We want to grab all columns with names that contain the word \"Diameter,\" so we set axis=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diameter=point_stats_matrix.filter(regex=\"Diameter\", axis=1)\n",
    "diameter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Plotting our DataFrame for further exploration <a name=\"plotting2\">\n",
    "\n",
    "Next let's try some simple visualization, starting with a histogram of area measurements for the spots. Remember that we already imported `seaborn` and `matplotlib` above in order to use plotting functions contained in these modules, so we don't need to import them again before using the functions below. As a reminder, when we imported, we abbreviated the `seaborn` module as `sns`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(point_stats_matrix.Area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a boxplot of Area values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"Area\", data=point_stats_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Take a look at the help for `lmplot` below and make a scatterplot comparing `Intensity Max Z` on the x axis against `Volume` on the y axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(sns.lmplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='Intensity Max Z', y='Volume', fit_reg=False, data=point_stats_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Filtering <a name=\"filtering\"/>\n",
    "\n",
    "Next we will discuss filtering. The scatterplot you created in the exercise above shows a smattering of points with an unsually large volume. Perhaps we decide that we don't trust that these are isolated points and therefore should exclude these outliers from our dataset. To do this, we will create a DataFrame named `filtered_points` that only includes spots with a volume less than 0.8. \n",
    "\n",
    "We do this by first defining a *mask* that will only return the data we are interested in. Our *mask* will remove those rows in the data that do not meet our criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = point_stats_matrix.Volume <= 0.8\n",
    "filtered_points=point_stats_matrix[mask]\n",
    "filtered_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another scatterplot to confirm that our filter worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#space for new scatterplot\n",
    "\n",
    "sns.lmplot(x='Intensity Max Z', y='Volume', fit_reg=False, data=filtered_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is the output of `point_stats_matrix.Volume <= 0.8`? Try it out by running the below cell. \n",
    "    \n",
    "How does this output help us to select the rows we want out of `point_stats_matrix`? (Hint: Think about what `True` and `False` mean in this context. Does it mean we want to return that row or not?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix.Volume <= 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Filter the `points_stats_matrix` dataset to have `Intensity Center X > 10000` and assign the result to `psm10000`. (Because of the spaces in the variable names, you will have to use `points_stats_matrix['Intensity Center X']` to access the column).\n",
    "\n",
    "Re-do the scatter plot of X and Y Intensity Centers to confirm that your filtering worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##space for your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Discussion Questions <a name=\"discuss2\"/>\n",
    "\n",
    "1. What is the difference between `groupby()` and filtering? When would you want to use each of these?\n",
    "2. What is the difference between filtering and indexing? \n",
    "\n",
    "## 2.15 What you learned today <a name=\"summary\"/>\n",
    "\n",
    "Congrats for getting this far! You have seen lots of features of `pandas` and `seaborne` that let you manipulate the data and visualize it. In particular:\n",
    "\n",
    "1. Basic `DataFrame`s\n",
    "1. `group_by`\n",
    "2. Boxplots and Scatterplots\n",
    "4. Pivoting data\n",
    "5. Filtering\n",
    "6. Merging\n",
    "\n",
    "There's a lot more about `DataFrames` to learn! Here's some good resources to learn even more:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix (Optional)\n",
    "\n",
    "The following sections are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting\n",
    "\n",
    "Faceting is one of the most powerful ways of exploring data. For example, we can see whether there is a `Strain` by `Sex` effect by producing *conditional* boxplots, which separate the data out by condition. This kind of visualization can be critical when exploring the data and looking for conditional effects. For example, we may notice a difference between Strains only for females, and not males.  \n",
    "\n",
    "Here we plot the distribution of weights conditioned on two variables: `Strain` and `Sex`. Note that these conditions are categorical variables, not numeric variables. \n",
    "\n",
    "One thing to notice is that we are passing in arguments to the `distplot` function within `g.map` as arguments to `g.map`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##use sns.FacetGrid to define our facets\n",
    "g = sns.FacetGrid(mousedata, col=\"Strain\", row=\"Sex\")\n",
    "##then we can define our plots\n",
    "g = g.map(sns.distplot, \"Weight\", bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DataFrames\n",
    "\n",
    "Another powerful way of manipulating data in Pandas is to combine two `DataFrame`s together. In fact, we actually merged two `DataFrame`s together to get the original `point_stats` `DataFrame`. Let's load in the two separate `DataFrame`s so we can see how we did this. Our new relative path is `data/raw-data/`, since that is where the unmerged data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## we have to use pd.read_table instead of pd.read_csv because the data\n",
    "## is tab delimited\n",
    "points_value = pd.read_table(\"data/raw-data/points-statistics-value.txt\", header=None)\n",
    "points_value.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no column names in the file, we have to add them manually by setting the `columns` property. How does `.columns` know which columns to rename to our new column names? We pass it a list whose values are the new column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_col_names = [\"ID_Time\",\"ID_Object\",\"ID_StatisticsType\",\"Value\"]\n",
    "points_value.columns = new_col_names\n",
    "points_value.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open the `points-statistics-type.txt` as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points_type = pd.read_table(\"data/raw-data/points-statistics-type.txt\", header=None)\n",
    "\n",
    "new_col_names = [\"ID_StatisticsType\", \"ID_Category\", \"ID_FactorList\", \"Name\", \"Unit\"]\n",
    "points_type.columns = new_col_names\n",
    "\n",
    "points_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh. We notice that both `points_type` and `points_value` have a column called `ID_StatisticsType`. Can we use this to combine the two tables? Yes! There's a function called `.merge()` in `pandas` that will let us do it.\n",
    "\n",
    "Note we're using a function from `pd`, and not from the `DataFrame` to do this. `pd.merge` takes four arguments:\n",
    "\n",
    "1. `left` - the left side table, which is `points_value`.\n",
    "2. `right` - the right side table, which is `points_type`.\n",
    "3. `left_on` - the column name in the left table that we want to merge on, which is `\"ID_StatisticsType\"`.\n",
    "4. `right_on` - the column name in the right table that we want to merge on, which is `\"ID_StatisticsType\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_table = pd.merge(left=points_value, right=points_type, \n",
    "         left_on = \"ID_StatisticsType\", right_on=\"ID_StatisticsType\")\n",
    "\n",
    "merged_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data out\n",
    "\n",
    "What if you want to save the `point_stats_matrix` DataFrame as its own csv file? Try running the code below. Where did it write the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix.to_csv(\"data/point_stats-mod.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also support for reading and writing Excel files if you need it: http://pandas.pydata.org/pandas-docs/stable/io.html#excel-files"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

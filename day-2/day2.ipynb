{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2: Diving Deeper into Pandas\n",
    "<img src=\"https://pbs.twimg.com/media/B_1KzLlUYAIFadB.jpg:large\" width=\"200\" height=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Day 2. Python programmers often organize their code into chunks called modules. A module contains a set of related commands to accomplish a task. Today we are going to learn two of the most useful modules for data manipulation and plotting, `pandas` and `seaborn`. \n",
    "\n",
    "First we will use a simple data set on mouse weights taken from males and females of different strains to illustrate how to obtain simple descriptive statistics, group the data, and plot. \n",
    "\n",
    "Then, we will looking some imaging statistics data from imaris and explore some more complex operations on the data.\n",
    "\n",
    "## 2.0 importing in `pandas` and `numpy`\n",
    "\n",
    "To start, we'll load up two modules with the `import` statement. The first is the `pandas` module, which will let us manipulate 2D tables. We refer to the `pandas` module here as `pd` as an abbreviation. The second module we load is the `numpy` module, mostly for the arithmetic functions built into it. We will do much more with the `numpy` module in Days 3 and 4.\n",
    "\n",
    "When you load a module using `import`, all of the functions available, such as `np.mean` are now accessible to you. Modules and import statements help programmers avoid naming conflicts because you can use short, straightforward names for functions and variables without worrying that they're already taken. Matlab does not have anything equivalent to Python's module system and therefore can be harder to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##Once imported, you can refer to the modules by `np`. \n",
    "##For example, we can take the mean of a list:\n",
    "np.mean([2,4,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Paths and Loading Data using `read_csv`\n",
    "\n",
    "Before we even load the data, we need to give Python an idea of where the data is. Our data is in the `data` directory in `day2`. We're going to specify the `path` or the folder location. There are two kinds of paths: *relative*, which is relative to our current directory, and *absolute*: the complete location of the directory. For example, the absolute path to the `data` folder on my Mac is: `/Users/laderast/Code/python_neurobootcamp/day2/data/`. \n",
    "\n",
    "The current directory for a Jupyter Notebook is always the directory where the notebook is located in. Since this notebook is in the `day2` directory, that is our current directory. All relative paths will be relative to this directory. \n",
    "\n",
    "**Note**: if you can, it's good practice to keep your Jupyter Notebook and your data in the same folder, and use relative paths to access everything. If someone wants to reproduce your analysis, they will be able to rerun everything in your notebook more easily. This makes your code more *reproducible*, which is really important these days. \n",
    "\n",
    "Okay, let's get to work! Let's load the data up using the `pd.read_csv` function. The `read_csv` function is a part of the `pandas` module, so we have to include the `pd.` in front of it so the computer knows to look in the `pandas` module to find and use this function. By using `pd.read_csv`, we return what is called a `pandas` `DataFrame`.   Most of our manipulations and plotting are going to be done on this `DataFrame`.\n",
    "\n",
    "A `DataFrame` can be thought of as a 2D table, but the values within each of the columns must be the same datatype. For example, any entry in the `MouseID` column must be a `string`, and any entry in the `Weight` column must be a `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mousedata = pd.read_csv(\"data/mouseData.csv\")\n",
    "##show the whole DataFrame\n",
    "mousedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Basic DataFrame manipulations\n",
    "\n",
    "The first thing to do with our `DataFrame` is to look at the first few rows of the function using `head()`. We often do this just to confirm that we loaded the data correctly (that it has the correct column names).\n",
    "\n",
    "**Note**: the `.` notation lets us access built in functions that are defined for the `DataFrame`. You can see all of the functions that you can do to a `DataFrame` by typing `mousedata.` into a code cell and hitting the `Tab` key. There's lots of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mousedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really useful function that we can immediately call on `mousedata` is `describe`. `describe` will return summary statistics on the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mousedata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful thing to note is the `.shape`, which returns the dimensions of the `DataFrame`. This can also be useful in confirming that we loaded the data in correctly.\n",
    "\n",
    "Note that we don't use `()` after `.shape`. This is because `shape` is a `Property` and not a function. This can be really confusing at first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mousedata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we'd like to get the number of rows in our `DataFrame`. Since `mousedata.shape` is a tuple (a special kind of list), we can access it using `[0]`. This can be really useful when we need to do something over all of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##get number of rows in mousedata\n",
    "mousedata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would we get the number of columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Put your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 A Quick Intro to functions\n",
    "\n",
    "Before we can move forward, we need to talk about functions for a little bit. You can think of a function as a bit of reusable code. The important thing is that you need to define the inputs (what goes into the function) and the output (what comes out of the function).\n",
    "\n",
    "Try and run the following function. What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##always begin with \"def\" when defining a new function, \n",
    "##have an interface defined in the \"()\", \n",
    "##and the definition ends in \":\"\n",
    "def square_x(x):\n",
    "    out = x * x\n",
    "    return out\n",
    "\n",
    "square_x(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look a bit more closely at how the `square_x` function is written. It begins with the word `def` (short for define) followed by the name of the function, a variable named in parenthesis, and a colon. The variable is the input to the function. The colon is also a necessary part of the function definition, and it begins the code block that defines what the function does.\n",
    "\n",
    "The rest of the function consists of this code block. In Python, this block must be indented with either tabs or spaces. The last line in the block contains the word 'return' followed by a variable name. This variable is the output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make a new function called `cube_y` that takes `y` as an input, and returns the cube of `y`. \n",
    "\n",
    "Run `cube_y(2)` to test out your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## space for your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, we actually will want multiple inputs to our function, so we can do this by supplying more inputs to our function interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mult_xy(x, y):\n",
    "    out = x * y\n",
    "    return out\n",
    "\n",
    "mult_xy(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Grouping\n",
    "\n",
    "Let's get back to our `DataFrame`. We'll define a function and use it using `groupby()`.  `groupby()` will group a `DataFrame` by one or more columns, and let you iterate through each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_mouse = mousedata.groupby(['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##return only those mice with Sex = \"M\"\n",
    "group_mouse.get_group('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you do with `groupby()`? One way to use it is to get *aggregate measures* based on group. For example, if we wanted to get the mean weight by gender, we can use the `apply` method on our data frames to return this. First we define a simple function called `mean_x` that returns the mean (we could have just used `np.mean` here, but it makes the code a little easier to understand).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_x(x):\n",
    "    return np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the `apply()` function to get the mean by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mousedata.groupby(['Sex']).apply(mean_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the `apply()` function takes a function as input and applies it to every element, in this case male and female weights.\n",
    "\n",
    "**Question**: why did the `groupby` only return `Weight`? Does it make sense to do `mean_x` on our `Strain` variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Define a function to calculate the standard deviation (in numpy the function you need will be called `np.std`) and apply it to return the standard deviation of weights by `Strain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Space for your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Plotting\n",
    "\n",
    "Let's look at some ways to visualize our `DataFrame`. We are going to use a module called `seaborn` to do our plotting, because the default plot options are pretty good, so we have to do less customization of our plots. In the next day, we'll see the module that `seaborn` is based on, `matplotlib`. \n",
    "\n",
    "Let's just plot the distribution of weights as a histogram. How many bins does our histogram have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##we need this line in our notebook to make matplotlib/seaborn work with Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of weights\n",
    "sns.distplot(mousedata.Weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Look up the help for `sns.distplot`. Note that there is a long list of input variables that are set as equal to `None` or `False`. This means that these are optional input variables that, unless defined, will run at their default definitions.\n",
    "\n",
    "To practice utilizing these optional inputs, change the number of bins to 40 in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(sns.distplot)\n",
    "\n",
    "## Space for your answer here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Boxplots\n",
    "\n",
    "Boxplots are super useful for looking at grouped means. Here we use the `sns.boxplot` function and group by `Sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "sns.boxplot(x = \"Sex\", y=\"Weight\", data=mousedata)\n",
    "\n",
    "# Set title with matplotlib\n",
    "plt.title('Mouse weight by sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "Create boxplots showing the weight data measured from the 2 different strains, B6 and D2. Make sure to add a title to your plot, such as \"Weight by Strain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Space for your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Faceting\n",
    "\n",
    "Faceting is one of the most powerful ways of exploring data. For example, we can see whether there is a `Strain` by `Sex` effect by producing *conditional* boxplots, which separate the data out by condition.\n",
    "\n",
    "Here we plot the distribution of weights conditioned on two variables: `Strain` and `Sex`. Note that these conditions are categorical variables, not numeric variables. \n",
    "\n",
    "One thing to notice is that we are passing in arguments to the `distplot` function within `g.map` as arguments to `g.map`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##use sns.FacetGrid to define our facets\n",
    "g = sns.FacetGrid(mousedata, col=\"Strain\", row=\"Sex\")\n",
    "##then we can define our plots\n",
    "g = g.map(sns.distplot, \"Weight\", bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complicated example\n",
    "\n",
    "We're going to do much more manipulation and visualization with `pandas` using data taken from Imaris. Imaris is image analysis software with many sophisticated functions. Below is a confocal image taken of inner hair cells stained with antibodies against *CtBP2* (a pre-synaptic ribbon marker), *GluR2* (a post-synaptic receptor) and *MyosinVIIA* (which labels the entire hair cell). There are three color channels (red, green, and blue) which indicate the intensity of the staining for *CtBP2*, *GluR2* and *MyosinVIIa*, respectively.\n",
    "\n",
    "\n",
    "## The data\n",
    "\n",
    "Up to 25 auditory nerve fibers synapse onto individual inner hair cells in\n",
    "normal-hearing individuals. However, these synapses can be permanently lost due\n",
    "to aging, exposure to noise or ototoxic drugs.  In experiments that study\n",
    "hearing loss, we need a way of quantifying the number of synapses per inner\n",
    "hair cell.\n",
    "\n",
    "One approach is to dissect the cochlea out of the experimental animals and use\n",
    "whole-mount immunohistochemistry to label the tissue with antibodies for\n",
    "pre-synaptic ribbons (CtBP2), post-synaptic receptors (GluR2) and cytoskeleton\n",
    "(Myosin VIIa). The distribution of these proteins can be captured by taking a series of\n",
    "two-dimensional images at various depths in the tissue.  These images are then\n",
    "\"stacked\" to create a three-dimensional image known as a Z-stack (since the\n",
    "third dimension is commonly referred to as the Z-axis).\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>1A. CtBP2 (pre-synaptic ribbon)</td>\n",
    "\t\t\t<td>1B. GluR2 (post-synaptic glutamate receptor)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"../day-4/data/CtBP2.png\" /></td>\n",
    "\t\t\t<td><img src=\"../day-4/data/GluR2.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "## The problem\n",
    "\n",
    "A functional inner hair cell synapse requires both a pre-synaptic ribbon and a\n",
    "post-synaptic glutamate receptor. The next step in our analysis is to determine\n",
    "whether each CtBP2 puncta is near a GluR2 label. \n",
    "\n",
    "This dataset was analyzed using Imaris to identify all CtBP2 puncta (white dots\n",
    "in fig. 2a). If you look closely at the composite (fig. 2b), you'll see that\n",
    "not all puncta have a glutamate receptor patch next to them (fig. 2b)! We\n",
    "should not be counting these for the purpose of analysis. So, we need to find a\n",
    "way to detect these false hits and eliminate them.\n",
    "\n",
    "<table>\n",
    "\t<body>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>A. CtBP2 puncta</td>\n",
    "\t\t\t<td>B. CtBP2 puncta overlaid on GluR2</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><img src=\"../day-4/data/CtBP2+points.png\" /></td>\n",
    "\t\t\t<td><img src=\"../day-4/data/CtBP2+GluR2+points.png\" /></td>\n",
    "\t\t</tr>\n",
    "\t</body>\n",
    "</table>\n",
    "\n",
    "One approach is to extract a fixed volume around each CtBP2 puncta (e.g., a 1um\n",
    "cube) and quantify the amount of GluR2 label in the volume. But, we don't know\n",
    "very much about the format of the data. We need to do a little exploration first.\n",
    "\n",
    "We used Imaris to detect all the \"spots\" in the CtBP2 (red) channel and compute some statistics about these spots. We've extracted the statistics file from the Imaris file into `csv` format just to make things easier. Just know that there are routines to extract this information from the file. Today we will practice exploring and visualizing the Imaris data and on Day 4 we will return to the question of how to quantify the amount of GluR2 label in a fixed volume around each CtBP2 puncta.\n",
    "\n",
    "Ok, let's load in the statistics for the CtBP2 puncta that were calculated by Imaris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats = pd.read_csv(\"data/points_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Exploring the Imaris Statistics\n",
    "\n",
    "Because this data file was automatically generated by Imaris, we first need to figure out how it is organized.\n",
    "\n",
    "We can start taking a look at the first few rows of our summary table using `point_stats.head()`. In general, this is a really good practice to get into. Sometimes our data may have a header or not, and we may have loaded the data incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Show first few rows\n",
    "point_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some things we notice? Well, there appear to be some data that describe the entire sample (such as \"Total Number of Spots\") as well as data for localized points identified by Imaris in the red channel (such as \"Area\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##show last few rows\n",
    "point_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##show dimensions of data frame\n",
    "point_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that attributes for the various traits describing a given spot (such as \"Area\" and \"Volume\") are not columns, but rather listed under the categorical column \"Name.\" If we are curious to see this full list of names, use the unique() function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_stats.Name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back to the first few rows of data, it appears that ID_Object of -1 designates statistics that describe the entire sample. Let's confirm by viewing all rows with ID_Object of -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##we will talk more about the point_stats[\"ID_Object\"]==-1 notation in\n",
    "##section \n",
    "point_stats[point_stats[\"ID_Object\"]==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all of the statistics that were collected for a single spot identified by Imaris, starting with the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_stats[point_stats[\"ID_Object\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the raw data for Diameter of spots in the X dimension (\"Diameter X\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_stats[point_stats[\"Name\"]==\"Diameter X\"].head(20)\n",
    "#OR\n",
    "point_stats[point_stats[\"ID_StatisticsType\"]==237].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We use `loc()` when we want to access things by *name* or *index* (it's position in our `DataFrame`). \n",
    "\n",
    "Remember when we were slicing the strings in Day 1? We use a very similar notation to access rows and columns in our `DataFrame`. Because the `DataFrame` has two dimensions, we need to specify two sets of positions to `loc`: column positions and row positions. We pass this on as a list of `row_positions` and `column_positions`.\n",
    "\n",
    "```\n",
    "point_stats.loc([row_positions, column_positions])\n",
    "```\n",
    "\n",
    "Let's use this notation to grab only the columns we want. Say we only want to view the `ID_Object`, `Value`, and `Name` columns from our `DataFrame`.\n",
    "\n",
    "Here we define a list called `colnames` and then pass as an argument into `.loc()`. Since we want all of the associated rows, we can use the `:` as an input to `row_positions`. `:` says, return all the values for the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames = [\"ID_Object\", \"Value\", \"Name\"]\n",
    "col_selected_data = point_stats.loc[:,colnames]\n",
    "selected_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to refer to these by the position in the dataframe. For example, say we want the first, second, and fifth columns in our dataset. Remembering that indexing always starts at 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_index = [0,1,4]\n",
    "\n",
    "col_selected_data = point_stats.iloc[:,col_index]\n",
    "\n",
    "col_selected_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use our `mean_x` function to return the mean `Intensity Max X` across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Space for your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Pivoting\n",
    "\n",
    "Now let's create a DataFrame that is more intuitive in terms of viewing the statistics Imaris has collected for each identified spot in the red channel. We will call this DataFrame `point_stats_matrix`. To do this, use the `pivot()` function, which reshapes data based on column values. This function is extremely useful in transforming data from *long* format to *wide* format. \n",
    "\n",
    "The `pivot` method takes three arguments: `index`, which you can think of as being the rows of the data, `columns`, which specify what columns should exist in the data, and `values`, which are the actual numerical values we want in each Cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix = point_stats.pivot(index='ID_Object', columns='Name', values='Value')\n",
    "point_stats_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the statistics for the entire data set (including \"Number of spots per time point\" and \"Total number of spots\") have an ID_Object of -1. Let's remove this row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_stats_matrix = point_stats_matrix.drop(-1)\n",
    "point_stats_matrix.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Plotting our DataFrame for further exploration\n",
    "\n",
    "Next let's try some simple visualization, starting with a histogram of area measurements for the spots. Remember that we already imported `seaborn` and `matplotlib` above in order to use plotting functions contained in these modules, so we don't need to import them again before using the functions below. As a reminder, when we imported, we abbreviated the `seaborn` module as `sns`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(point_stats_matrix.Area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a boxplot of Area values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"Area\", data=point_stats_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Take a look at the help for `lmplot` below and make a scatterplot comparing `Intensity Max Z` on the x axis against `Volume` on the y axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(sns.lmplot)\n",
    "\n",
    "?sns.kdeplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(x='Intensity Max Z', y='Volume', data=point_stats_matrix)\n",
    "sns.lmplot(x='Intensity Max Z', y='Volume', fit_reg=False, data=point_stats_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Filtering\n",
    "\n",
    "Next we will discuss filtering. The scatterplot you created in the exercise above shows a smattering of points with an unsually large volume. Perhaps we decide that we don't trust that these are isolated points and therefore should exclude these outliers from our dataset. To do this, we will create a DataFrame named `filtered_points` that only includes spots with a volume less than 0.8. \n",
    "\n",
    "We do this by first defining a *mask* that will only return the data we are interested in. Our *mask* will remove those rows in the data that do not meet our criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = point_stats_matrix.Volume <= 0.8\n",
    "filtered_points=point_stats_matrix[mask]\n",
    "filtered_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another scatterplot to confirm that our filter worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#space for new scatterplot\n",
    "\n",
    "sns.lmplot(x='Intensity Max Z', y='Volume', fit_reg=False, data=filtered_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is the output of `point_stats_matrix.Volume <= 0.8`? Try it out by running the below cell. \n",
    "    \n",
    "How does this output help us to select the rows we want out of `point_stats_matrix`? (Hint: Think about what `True` and `False` mean in this context. Does it mean we want to return that value or not?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_stats_matrix.Volume <= 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Filter the `points_stats_matrix` dataset to have `Intensity Center X > 10000` and assign the result to `psm10000`. (Because of the spaces in the variable names, you will have to use points_stats_matrix['Intensity Center X'] to access the column).\n",
    "\n",
    "Re-do the scatter plot of X and Y Intensity Centers to confirm that your filtering worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##space for your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Merging DataFrames\n",
    "\n",
    "Another powerful way of manipulating data in Pandas is to combine two `DataFrame`s together. In fact, we actually merged two `DataFrame`s together to get the original `point_stats_matrix` `DataFrame`. Let's load in the two separate `DataFrame`s so we can see how we did this. Our new relative path is `data/raw-data/`, since that is where the unmerged data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## we have to use pd.read_table instead of pd.read_csv because the data\n",
    "## is tab delimited\n",
    "points_value = pd.read_table(\"data/raw-data/points-statistics-value.txt\", header=None)\n",
    "points_value.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no column names in the file, we have to add them manually by setting the `columns` property. How does `.columns` know which columns to rename to our new column names? We pass it a list whose values are the new column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_col_names = [\"ID_Time\",\"ID_Object\",\"ID_StatisticsType\",\"Value\"]\n",
    "points_value.columns = new_col_names\n",
    "points_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open the `points-statistics-type.txt` as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points_type = pd.read_table(\"data/raw-data/points-statistics-type.txt\", header=None)\n",
    "\n",
    "new_col_names = [\"ID_StatisticsType\", \"ID_Category\", \"ID_FactorList\", \"Name\", \"Unit\"]\n",
    "points_type.columns = new_col_names\n",
    "\n",
    "points_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh. We notice that both `points_type` and `points_value` have a column called `ID_StatisticsType`. Can we use this to combine the two tables? Yes! There's a function called `.merge()` in `pandas` that will let us do it.\n",
    "\n",
    "Note we're using a function from `pd`, and not from the `DataFrame` to do this. `pd.merge` takes four arguments:\n",
    "\n",
    "1. `left` - the left side table, which is `points_value`.\n",
    "2. `right` - the right side table, which is `points_type`.\n",
    "3. `left_on` - the column name in the left table that we want to merge on, which is `\"ID_StatisticsType\"`.\n",
    "4. `right_on` - the column name in the right table that we want to merge on, which is `\"ID_StatisticsType\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_table = pd.merge(left=points_value, right=points_type, \n",
    "         left_on = \"ID_StatisticsType\", right_on=\"ID_StatisticsType\")\n",
    "\n",
    "merged_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Getting data out\n",
    "\n",
    "What if you want to save the `point_stats_matrix` DataFrame as its own csv file? Try running the code below. Where did it write the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_stats_matrix.to_csv(\"data/point_stats-mod.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also support for reading and writing Excel files if you need it: http://pandas.pydata.org/pandas-docs/stable/io.html#excel-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Discussion questions\n",
    "\n",
    "1. What is the difference between `groupby()` and filtering? When would you want to use each of these?\n",
    "2. What is the difference between filtering and indexing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 What you learned today\n",
    "\n",
    "Congrats for getting this far! You have seen lots of features of Pandas and Seaborne that let you manipulate the data and visualize it. \n",
    "\n",
    "1. `group_by`\n",
    "2. Filtering\n",
    "3. Boxplots and Scatterplots\n",
    "4. Faceting\n",
    "5. Pivoting data\n",
    "\n",
    "There's a lot more about `DataFrames` to learn! Here's some good resources to learn even more:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
